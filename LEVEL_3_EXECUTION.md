# Уровень 3. Процесс исполнения и оркестрации тестов

## 1. Скрипт `scripts/run_all_benchmarks.sh`: центральный оркестратор

### 1.1 Инициализация
* Первые строки определяют ANSI-коды цветов (`GREEN`, `BLUE`, `YELLOW`, `RED`, `NC`) для форматирования вывода.
* Далее инициализируются флаги-переменные, отражающие виды тестов (`RUN_COMPUTATIONAL`, `RUN_HTTP`, `RUN_ASYNC`, `RUN_COLD_START`) и режим «запустить всё» (`RUN_ALL`).
* `RUNTIMES` — динамический массив рантаймов, по умолчанию вся тройка: Node.js, Deno, Bun.
* `ITERATIONS` по умолчанию = 30.
* `VERBOSE` регулирует подробность логов.

### 1.2 Парсинг аргументов CLI
Аргументы обрабатываются циклом `while [[ $# -gt 0 ]]` с `case`-разбором:
| Опция | Код парсинга | Назначение |
|-------|--------------|------------|
| `-a`, `--all` | `RUN_ALL=true` | Включает все группы тестов и все рантаймы. |
| `-c`, `--computational` | `RUN_COMPUTATIONAL=true` | Вычислительные тесты. |
| `-h`, `--http` | `RUN_HTTP=true` | HTTP-серверы. |
| `-s`, `--async` | `RUN_ASYNC=true` | Асинхронные серверы. |
| `-d`, `--cold-start` | `RUN_COLD_START=true` | Холодный старт. |
| `-r`, `--runtime <rt>` | `RUNTIMES+=("$2")` | Ограничивает список рантаймов. Опция может повторяться. |
| `-i`, `--iterations <n>` | `ITERATIONS="$2"` | Кол-во итераций, пробрасывается во внутренние скрипты. |
| `-v`, `--verbose` | `VERBOSE=true` | Включает детальный лог. |
| `--help` | вызов `show_help` | Выводит справку и примеры. |

Дополнительная логика после парсинга:
* Если задан `--all`, принудительно выставляются все *_true* флаги.
* Если не выбрано ни одной группы, выводится красная ошибка и скрипт завершается.
* Если список рантаймов пуст, он заполняется значениями `node deno bun`.

### 1.3 Подготовка окружения
```
cd "$(dirname "$0")/.."      # переход в корень репо
mkdir -p ./results              # единое хранилище артефактов
```

### 1.4 Вспомогательные функции
* **`log(msg, ["important"])`** — печатает строку c таймстампом `[%F %T]` только если `VERBOSE=true` _или_ вторым аргументом передано `important`.
* **`show_run_info()`** — однократный синяя-шапка с выбранной конфигурацией.
* **`run_*_benchmarks()`** — четыре однотипные функции-обёртки, вызывающие специализированные скрипты и проверяющие `$?`.

Пример для вычислительных:
```bash
bash ./scripts/run_computational_benchmarks.sh -r $runtime -i $ITERATIONS
if [ $? -ne 0 ]; then
  log "${RED}Ошибка …${NC}" "important"
fi
```

### 1.5 Основной поток (`main`)
1. Выводит конфигурацию (`show_run_info`).
2. Запоминает `START_TIME=$(date +%s)`.
3. Последовательно вызывает выбранные группы `run_…` (между ними `echo ""` для визуального разделения).
4. После выполнения всех групп вычисляет длительность и печатает финальную сводку:
   ```bash
   DURATION=$((END_TIME-START_TIME))
   echo "Общее время: $(($DURATION / 60)) мин $(($DURATION % 60)) сек"
   ```
5. Результаты каждого подскрипта сохраняются в под-директориях `./results/*`.

## 2. Специализированные скрипты

### 2.1 `scripts/run_computational_benchmarks.sh`
* Поддерживает флаги `--all`, `--test <fibonacci|sorting|matrix|json>`, `--runtime`, `--iterations`.
* Определяет массив `RUNTIMES_TO_RUN` и формирует строку `ARGS` для дочерних скриптов `benchmark-suites/<rt>/computational/run_computational_benchmarks.sh`.
* Запускает рантаймы **последовательно**; между ними 10-секундная пауза для стабилизации.
* Код возврата каждого дочернего скрипта проверяется, ведётся счётчик успехов/ошибок.
* В качестве итогов выводит зелёную статистику «Успешно/С ошибками».

### 2.2 `scripts/run_http_benchmarks.sh`
Принимает позиционные аргументы: `<runtime> [iterations]`.
1. Определяет `SERVICE`, `PORT` и путь к конкретному `Dockerfile`.
2. Запускает контейнер через `docker compose up -d $SERVICE`.
3. Пингует эндпоинт `/ping` до 10 раз (3 сек интервал).
4. Для каждой итерации (по умолчанию `1`, может быть увеличено внешними аргументами):
   * Запускает `wrk` (`-t4 -c100 -d30s`).
   * Парсит stdout в переменные latency / req-per-sec, обрабатывая постфиксы `k`.
   * Сериализует результат в `results/http/<runtime>_http_<timestamp>.json`; каждый итерационный объект добавляется в массив `iterations`.
5. После теста сервер останавливается `docker compose stop $SERVICE`.
6. Скрипт возвращает `exit $WRK_EXIT_CODE`, что позволяет `run_all_benchmarks.sh` определить ошибку.

### 2.3 `scripts/run_async_benchmarks.sh`
Идентичен HTTP-скрипту, но тестирует эндпоинт `/async-bench` и дополнительно делает `curl` запрос для извлечения пользовательской метрики `"duration_ms"` из JSON-ответа сервера.

## 3. Скрипты холодного старта `benchmark-suites/*/cold-start/cold_start_benchmark.sh`
Рассмотрим вариант Node.js (Bun и Deno устроены аналогично).

### 3.1 Обработка параметров
* Поддерживается `-i <iterations>` через `getopts`.
* Значение попадает в глобальную `ITERATIONS` (по умолчанию 30).

### 3.2 Проверка окружения и подготовка
1. `check_dependencies()` убеждается в наличии `docker`, `curl`, `bc`, `jq`, `python3` и доступе к Docker-daemon.
2. `build_image()` собирает `benchmark-suites/node/cold-start/Dockerfile` ➜ `IMAGE_NAME=node-cold-start-benchmark`.
3. `create_results_dir()` гарантирует наличие `results/cold-start`.

### 3.3 Измерительная логика одной итерации (`run_benchmark_iteration`)
1. Выставляет уникальное имя контейнера.
2. Фиксирует `T0` на хосте: `python3 -c 'import time; print(int(time.time()*1000))'`.
3. Запускает контейнер, прокидывая порт `-p $PORT:3000`.
4. В цикле 0.1 счита считывает логи `docker logs`, ожидая строки `READY_TIMESTAMP:<ms>` — это `T1`.
5. После готовности делает HTTP-запрос на `/`, фиксируя `T2`.
6. Вычисляет:
   * `startup_time_ms = T1 - T0` — время запуска рантайма.
   * `first_request_time_ms = T2 - T1`.
   * `total_cold_start_time_ms = T2 - T0`.
7. Записывает результат в массивы Bash и (через `jq`) добавляет объект итерации в результирующий JSON.
8. Контейнер удаляется.

### 3.4 Пост-обработка
После всех итераций `calculate_statistics()` формирует окончательный JSON со сводной информацией о версии Node, параметрах хост-системы (CPU, RAM, OS) и массивом `iterations`.

## 4. Проверка успешности подскриптов
В каждом вызове `bash ./scripts/…` оркестратор немедленно проверяет `$?`:
```bash
if [ $? -ne 0 ]; then
  log "${RED}Ошибка …${NC}" "important"
fi
```
Неуспешное завершение дочернего скрипта **не** прерывает выполнение остальных групп, но помечается красным сообщением.

## 5. Система логирования и формат вывода
| Цвет | Переменная | Использование |
|------|------------|---------------|
| Зелёный | `$GREEN` | Успешные завершения и финальная статистика. |
| Синий   | `$BLUE`  | Заголовки разделов ("=== Запуск … ==="). |
| Жёлтый  | `$YELLOW`| Выбранные параметры конфигурации. |
| Красный | `$RED`   | Ошибки. |

Функция `log()` добавляет префикс времени и выводит сообщение либо при `--verbose`, либо при пометке `important`. Таким образом:
```bash
[$(date "+%Y-%m-%d %H:%M:%S")] === Запуск HTTP тестов ===
```

### Структура вывода `run_all_benchmarks.sh`
1. Синяя шапка конфигурации.
2. Для каждой группы:
   * Синий заголовок «Запуск …».
   * Строки запуска рантаймов.
   * Зелёная строка об успехе или красная об ошибке.
3. Финальная синяя сводка с общим временем и путём к артефактам.

## 6. Примеры запуска (из блока `show_help`)
```bash
# Запуск всех бенчмарков всех рантаймов
./scripts/run_all_benchmarks.sh --all

# Только вычислительные и HTTP-тесты для Node.js
./scripts/run_all_benchmarks.sh -c -h -r node

# HTTP и async-тесты для Bun и Deno, по 10 итераций
./scripts/run_all_benchmarks.sh --http --async -r bun -r deno -i 10
```

Описание:
* Первая команда эквивалентна `-a` и активирует все типы тестов (`-c -h -s -d`) + все рантаймы.
* Вторая ограничивает типы тестов и рантайм.
* Третья демонстрирует множественные `--runtime` и переопределение `--iterations`.

## 7. Итоги
Сценарии проекта `runtimes-benchmarks` образуют многоуровневую систему оркестрации:
* **Уровень 1** — `run_all_benchmarks.sh` управляет выбором бенчмарков и ресурсов.
* **Уровень 2** — специализированные скрипты подготавливают Docker-сервисы, запускают нагрузочное тестирование `wrk` или собственные вычислительные задачи.
* **Уровень 3** — скрипты внутри `benchmark-suites/**` ведут непосредственный запуск и сбор метрик, формируя JSON-артефакты.

Такое разделение упрощает поддержку, делает поток выполнения прозрачным и позволяет легко расширять систему новыми видами тестов или рантаймами. 